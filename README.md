# ğŸ“š ReadBuddy

**ReadBuddy** is a lightweight Chrome extension that helps students understand dense terminology and context-specific phrases in research papers using **AI-powered explanations**.  
Get instant, plain-English insights **without leaving the paper** â€” making academic reading faster and more accessible.

---

## ğŸ¯ Problem

Students often struggle with:

- Dense academic terminology in research papers  
- Context-specific phrases that require domain knowledge  
- Constantly switching between papers and search engines  
- Loss of reading flow and comprehension  

---

## ğŸ’¡ Solution

**ReadBuddy** provides fast, context-aware explanations directly in your browser:

1. Highlight any phrase in a research paper  
2. Right-click â†’ **â€œExplain with AIâ€**  
3. Get a short, plain-English explanation instantly  
4. Choose:
   - ğŸ§© **Explain further** â€“ get more detailed insight  
   - ğŸ’¬ **Ask follow-up** â€“ open a mini chat  
   - âœ… **Got it** â€“ dismiss  

No context switching. No interruptions. Just seamless learning.

---

## âœ¨ Features

- âš¡ **Instant Explanations:** Highlight â†’ right-click â†’ understand  
- ğŸ§  **Plain English:** 2â€“3 sentence, clarity-optimized explanations  
- ğŸ” **Follow-up Options:** â€œExplain furtherâ€, â€œAsk follow-upâ€, â€œGot itâ€  
- ğŸ” **Context-Aware (RAG):** Uses document context for accuracy  
- ğŸ¤– **Multi-LLM Support:** Works with OpenAI, Anthropic, or local models  
- ğŸª¶ **Fast & Lightweight:** Minimal UI, maximum speed  

---

## ğŸ—ï¸ Architecture

### How It Works

```text
[User highlights text in webpage]
        â†“
[Right-click â†’ "Explain with AI"]
        â†“
[Extension captures selection + page context]
        â†“
[POST request â†’ FastAPI Backend]
        â†“
[Backend retrieves relevant context (RAG)]
        â†“
[LLM generates explanation]
        â†“
[Response streamed back to extension]
        â†“
[Floating card appears with explanation]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Explanation (2-3 sentences) â”‚
â”‚ [Explain further]           â”‚
â”‚ [Ask follow-up]             â”‚
â”‚ [Got it]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§° Tech Stack

### **Frontend / Extension**
- Chrome Extension (Manifest V3)
- Vanilla JavaScript (content scripts, background service worker)
- CSS (for floating UI cards)

### **Backend**
- FastAPI (Python)
- OpenAI / Anthropic APIs
- Pinecone / ChromaDB (Vector Database for RAG)
- PostgreSQL (analytics and logging)
- Docker & Docker Compose

### **Optional Frontend Dashboard**
- Next.js
- Tailwind CSS
- Recharts (for analytics visualization)

---

## ğŸ“ Project Structure

```plaintext
readbuddy/
â”œâ”€â”€ backend/              # FastAPI backend service
â”‚   â”œâ”€â”€ routers/          # API route handlers
â”‚   â”œâ”€â”€ services/         # Business logic (LLM, RAG, embeddings)
â”‚   â”œâ”€â”€ models/           # Database models
â”‚   â””â”€â”€ main.py           # FastAPI entry point
â”œâ”€â”€ extension/            # Chrome extension
â”‚   â”œâ”€â”€ manifest.json     # Extension configuration
â”‚   â”œâ”€â”€ background.js     # Service worker (context menu, API calls)
â”‚   â”œâ”€â”€ content.js        # Content script (UI injection)
â”‚   â””â”€â”€ popup.html        # Extension popup (optional)
â”œâ”€â”€ frontend/             # Next.js dashboard (optional)
â”œâ”€â”€ docker-compose.yml    # Docker orchestration
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ README.md
```

# ğŸš€ Getting Started

## ğŸ§© Prerequisites
- **Python 3.10+**
- **Node.js 18+** (for frontend dashboard, optional)
- **Docker & Docker Compose** (optional)
- **OpenAI or Anthropic API key**

---

## âš™ï¸ Backend Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/neehanayak/ReadBuddy.git
cd ReadBuddy
```
### 2ï¸âƒ£ Set up environment variables
```bash
cp .env.example .env
# Edit .env and add your API keys
```
3ï¸âƒ£ Run with Docker (recommended)
```bash
docker-compose up --build
```
4ï¸âƒ£ Or run locally
```bash
cd backend
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```
âœ… Verify the backend

- **API:** http://localhost:8000
- **Docs:** http://localhost:8000/docs
- **Health:** http://localhost:8000/health


## ğŸ§© Install Chrome Extension

1. Open **Google Chrome** and navigate to: 
```bash 
chrome://extensions/
```
2. Enable **Developer mode** (top right corner)
3. Click **Load unpacked**
4. Select the `extension/` folder from this repository

âœ… The **ReadBuddy** icon will now appear in your Chrome toolbar.

## ğŸª„ Usage

1. Open any **research paper** or **online article**  
2. Highlight a **phrase or term** you want explained  
3. Right-click â†’ select **â€œExplain with AIâ€**  
4. A floating card will appear with a **2â€“3 sentence plain-English explanation**

### ğŸ’¡ Available Options
- ğŸ§  **Explain further** â†’ Get a more detailed explanation  
- ğŸ’¬ **Ask follow-up** â†’ Open a mini chat for clarification  
- âœ… **Got it** â†’ Dismiss the explanation card


## ğŸ”§ Configuration

### ğŸ§¾ Environment Variables (`.env`)
Create a `.env` file in the root directory based on `.env.example` and fill in your API keys and configuration:

```bash
# LLM Configuration
LLM_PROVIDER=openai  # Options: openai, anthropic, local
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Vector Database
VECTOR_DB=pinecone  # Options: pinecone, chroma
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=your_pinecone_environment_here
PINECONE_INDEX_NAME=readbuddy

# Database
DATABASE_URL=postgresql://postgres:postgres@db:5432/readbuddy

# Server
BACKEND_PORT=8000
FRONTEND_PORT=3000

# CORS
ALLOWED_ORIGINS=http://localhost:3000,chrome-extension://*
```

## ğŸ“¡ API Endpoints

### âœ… Current Endpoints
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET    | `/`      | Root endpoint with service info |
| GET    | `/health`| Health check |

### ğŸ§© Planned Endpoints
| Method | Endpoint             | Description |
|--------|--------------------|-------------|
| POST   | `/query`             | Get AI explanation for highlighted text |
| POST   | `/ingest`            | Upload documents for RAG context |
| GET    | `/analytics/summary` | Retrieve usage statistics |

---

## ğŸ§ª Development

### ğŸ§  Run Tests
```bash
cd backend
pytest
```
### ğŸ§¹ Linting
```bash
flake8 backend
```
---
## ğŸš€ Build for Production
``` bash
docker-compose -f docker-compose.prod.yml up --build
```
## ğŸ—ºï¸ Roadmap
- âœ… Project setup and FastAPI backend  
- ğŸ§  LLM abstraction layer (OpenAI, Anthropic)  
- ğŸ” Vector database integration (RAG)  
- âš¡ Streaming responses  
- ğŸ§© Chrome extension (basic functionality)  
- ğŸ’¬ Follow-up chat interface  
- ğŸ“Š Analytics dashboard  
- ğŸ“š Multi-document context support  
- ğŸ›°ï¸ Offline mode (local LLMs)  
- ğŸ¦Š Firefox extension support  

---

## ğŸ¤ Contributing
Contributions are always welcome ğŸ’™

1. **Fork** the repository  
2. **Create your branch**
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. **Commit Changes**
```bash
git commit -m "Add some AmazingFeature"
```
4. **Push your branch**
```bash
git push origin feature/AmazingFeature
```
5. **Open a Pull Request**

--

## ğŸ“„ License
This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Author
**Neeha Nayak**  
ğŸ”— [GitHub Profile](https://github.com/neehanayak)

---

## ğŸ™ Acknowledgments
- Inspired by the need to make **academic research more accessible**  
- Built with â¤ï¸ using **FastAPI**, **OpenAI**, and modern web technologies  
- Thanks to all contributors and early testers who shaped the project  

---

## ğŸ“§ Contact
For questions or feedback, please open an **issue** or contact:  
ğŸ“© *wizardingbyte@outlook.com*

---

> Made with â¤ï¸ to help students read research papers more effectively.



